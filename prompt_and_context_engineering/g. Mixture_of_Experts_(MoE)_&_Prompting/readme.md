# **g. Mixture-of-Experts (MoE) & Prompting**

### **1. What is Mixture-of-Experts (MoE)?**

**Mixture-of-Experts (MoE)** is a machine learning architecture designed to improve the **efficiency and scalability** of large models (especially Large Language Models - LLMs).

Instead of using the entire model for every input, MoE activates only a few specialized parts called **experts**, depending on the input type.

This approach allows the model to perform better with **less computation**.

---

### **2. MoE Architecture**

The MoE architecture includes three key parts:

### ğŸ§  **Experts**

- These are **specialized sub-networks**, each trained for a specific type of data or task (e.g., math, reasoning, or writing).
- Example: One expert may handle logical reasoning while another handles creative writing.

### ğŸ”€ **Gating Network / Router**

- The **router** is a small neural network that **decides which experts to activate** for a given input.
- It assigns **probabilities or scores (metrics)** to each expert, showing how suitable each one is for the current input.

### ğŸ’¡ **Metrics, Dense and Sparse Vectors**

- The router calculates a **metric** (numerical score) for every expert using its learned parameters.
- All these scores form a **dense vector** (where every expert has a value).
- The model then keeps only the **top few highest scores** (top-k experts) and sets the rest to zero, forming a **sparse vector**.
- This step is called **sparse activation**, making the model fast and efficient.

### âš™ï¸ **Routing and Selection**

1. Input is sent to the router.
2. Router computes scores for all experts.
3. It selects the top-k experts with the highest probabilities.
4. Those experts process the input, and their outputs are **combined using weighted probabilities**.

 **Example:**

| Expert | Score (Metric) | Probability |
| --- | --- | --- |
| Expert 1 | 0.15 | 15% |
| Expert 2 | 0.70 | 70% |
| Expert 3 | 0.05 | 5% |

âœ… The router selects **Expert 2** (and sometimes the next best, Expert 1).

---

### **3. Benefits and Drawbacks**

| **Benefits** | **Drawbacks** |
| --- | --- |
| ğŸ’¨ **Efficiency:** Only a few experts are used each time, saving compute. | âš–ï¸ **Routing Instability:** Router may sometimes misroute or overload some experts. |
| ğŸ“ˆ **Scalability:** Can scale to trillions of parameters but run efficiently. | ğŸ§  **Memory Overhead:** Storing many experts requires more memory. |
| ğŸ¯ **Specialization:** Each expert becomes skilled in a specific domain. | ğŸ” **Interpretability Challenge:** Hard to see why a certain expert was chosen. |

---

### **4. Impact of MoE on Prompting**

MoE affects **prompt engineering** because the way you write your prompt can influence **which experts â€œwake upâ€** inside the model.

The **router selects experts** based on the tokens (words) in your prompt â€” so clear, domain-specific, and well-structured prompts are essential.

### ğŸ’¬ **How Prompting Changes in MoE Models**

| **Aspect** | **Change / Strategy** |
| --- | --- |
| ğŸ·ï¸ **Domain-specific signals upfront** | Start prompts with clear domain cues like â€œAs a financial analystâ€¦â€ to guide the router. |
| âœ‚ï¸ **Separate mixed tasks** | Avoid mixing different types of requests (e.g., coding + poetry). Handle one task per prompt or step. |
| ğŸ§­ **Front-load cues** | Mention the key task and style early â€” routers decide routing based on early tokens. |
| ğŸ” **Reduce temperature** | For consistent answers, set temperature = 0 to reduce randomness in expert selection. |
| ğŸ§  **Use few-shot examples** | Include domain-specific examples to help the router understand which experts to activate. |

---

### **5. Expert-Aware Prompting**

To make the best use of MoE models, follow these **expert-aware prompting** principles:

âœ… **Do:**

- Use explicit role and task definitions
    
    e.g., â€œRole: Data Scientist. Task: Analyze dataset trends.â€
    
- Use clear, domain-specific vocabulary
- Keep prompts concise and focused
- Match examples closely to the target task
- Specify language and style (e.g., â€œLanguage: Urdu. Style: formal.â€)

âŒ **Avoid:**

- Vague instructions (â€œyou know what I meanâ€)
- Overlong introductions that bury the real task
- Combining unrelated formats in one prompt (like â€œwrite code + poemâ€)

---

### **6. Why This Matters**

Because the **routerâ€™s decision depends on the prompt wording**, even small changes can **activate different experts** â€” which can change the output quality.

So, strong prompt engineering is not just about clarity, but also about **routing control** â€” helping the model pick the right internal specialists.

---

**Meaning:**

cues = signals, hints, or clues

---
